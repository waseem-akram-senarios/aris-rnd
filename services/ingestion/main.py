"""
FastAPI Entrypoint for ARIS Ingestion Service
Handles document upload, parsing, and indexing.
"""
import os
import uuid
import logging
import hashlib
import time as time_module
import asyncio
from datetime import datetime
from contextlib import asynccontextmanager
from typing import Optional, Dict, Any, List
from fastapi import BackgroundTasks, FastAPI, UploadFile, File, HTTPException, Form, Depends, Request
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv

from scripts.setup_logging import setup_logging
from shared.config.settings import ARISConfig
from shared.schemas import DocumentMetadata, ProcessingResult, FullIngestionRequest, FullIngestionResponse
from .engine import IngestionEngine
from .processor import DocumentProcessor
from shared.utils.sync_manager import SyncManager, get_sync_manager

logger = setup_logging(
    name="aris_rag.ingestion",
    level=logging.INFO,
    log_file="logs/ingestion_service.log"
)

load_dotenv()

# Global instances
engine: Optional[IngestionEngine] = None
processor: Optional[DocumentProcessor] = None
sync_manager: Optional[SyncManager] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan context manager for startup/shutdown"""
    global engine, processor, sync_manager
    
    logger.info("=" * 60)
    logger.info("[STARTUP] Initializing ARIS Ingestion Service")
    logger.info("=" * 60)
    
    # Initialize sync manager with service name for tracking
    sync_manager = get_sync_manager("ingestion")
    logger.info("âœ… [STARTUP] Sync Manager initialized")
    
    # Initialize engine with ingestion-relevant settings
    engine = IngestionEngine(
        vector_store_type=ARISConfig.VECTOR_STORE_TYPE,
        opensearch_domain=ARISConfig.AWS_OPENSEARCH_DOMAIN,
        opensearch_index=ARISConfig.AWS_OPENSEARCH_INDEX,
        chunk_size=ARISConfig.DEFAULT_CHUNK_SIZE,
        chunk_overlap=ARISConfig.DEFAULT_CHUNK_OVERLAP
    )
    
    # Initialize processor
    processor = DocumentProcessor(engine)
    
    # Force initial sync on startup
    sync_manager.force_full_sync()
    
    # Register callback to reload engine's index map on sync
    def on_sync(result):
        if engine and (result.get("index_map") or result.get("registry")):
            try:
                engine._load_document_index_map()
                logger.debug("[ingestion] Engine index map reloaded via sync callback")
            except Exception as e:
                logger.warning(f"[ingestion] Failed to reload index map in callback: {e}")
    
    sync_manager.register_sync_callback(on_sync)
    
    # Start background sync task for automatic synchronization
    try:
        loop = asyncio.get_event_loop()
        sync_manager.start_background_sync(loop)
        logger.info("âœ… [STARTUP] Background sync task started")
    except Exception as e:
        logger.warning(f"[STARTUP] Could not start async background sync: {e}")
        sync_manager._start_threaded_sync()
    
    logger.info("âœ… [STARTUP] Ingestion Service Ready")
    yield
    
    # Cleanup
    sync_manager.stop_background_sync()
    logger.info("[SHUTDOWN] Ingestion Service Shutting Down")

app = FastAPI(
    title="ARIS Ingestion Service",
    description="Microservice for document ingestion, parsing, and indexing",
    version="1.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.middleware("http")
async def auto_sync_middleware(request: Request, call_next):
    """Middleware to automatically sync state before and after operations."""
    request_id = request.headers.get("X-Request-ID", "internal")
    
    # Auto-sync before critical operations
    if sync_manager and request.url.path in ["/ingest", "/process", "/health", "/status", "/ingest/full"]:
        try:
            sync_manager.check_and_sync()
        except Exception as e:
            logger.debug(f"Auto-sync check failed in middleware: {e}")
    
    logger.info(f"Ingestion: [ReqID: {request_id}] {request.method} {request.url.path}")
    response = await call_next(request)
    response.headers["X-Request-ID"] = request_id
    
    # AUTOMATIC BROADCAST SYNC after document ingestion operations
    # Gateway is the sole sync coordinator. Ingestion excludes itself to avoid
    # circular callback (Gateway would otherwise call Ingestion /sync/force).
    if sync_manager and request.url.path in ["/ingest", "/process", "/ingest/full"] and response.status_code in [200, 201]:
        try:
            # First, sync locally (Ingestion's own state)
            sync_manager.instant_sync()
            
            # Ask Gateway to broadcast to retrieval + mcp (exclude ingestion)
            import httpx
            gateway_url = os.getenv("GATEWAY_URL", "http://127.0.0.1:8500")
            
            try:
                with httpx.Client(timeout=5.0) as client:
                    broadcast_response = client.post(f"{gateway_url}/sync/broadcast?exclude=ingestion")
                    if broadcast_response.status_code == 200:
                        logger.info(f"ðŸ“¡ [Ingestion] Auto-broadcast sync completed (self excluded)")
                    else:
                        logger.debug(f"ðŸ“¡ [Ingestion] Broadcast returned: {broadcast_response.status_code}")
            except Exception as broadcast_err:
                logger.debug(f"ðŸ“¡ [Ingestion] Broadcast failed (services may sync on next interval): {broadcast_err}")
                
        except Exception as e:
            logger.debug(f"Post-operation sync failed: {e}")
    
    return response

def get_processor() -> DocumentProcessor:
    if processor is None:
        raise HTTPException(status_code=500, detail="Processor not initialized")
    return processor

@app.get("/health")
async def health_check():
    """Health check with registry and index map sync verification"""
    try:
        # Verify document registry and index map are accessible
        from shared.config.settings import ARISConfig
        import os
        import json
        
        registry_path = ARISConfig.DOCUMENT_REGISTRY_PATH
        registry_accessible = os.path.exists(registry_path) or os.path.exists(os.path.dirname(registry_path))
        
        # Get document count from registry
        doc_count = 0
        try:
            if os.path.exists(registry_path):
                with open(registry_path, 'r') as f:
                    registry_data = json.load(f)
                    doc_count = len(registry_data) if isinstance(registry_data, dict) else 0
        except Exception as e:
            logger.debug(f"get_processor: {type(e).__name__}: {e}")
            pass
        
        index_map_path = os.path.join(ARISConfig.VECTORSTORE_PATH, "document_index_map.json")
        index_map_accessible = os.path.exists(index_map_path) or os.path.exists(ARISConfig.VECTORSTORE_PATH)
        
        # Get index map count
        index_map_count = 0
        try:
            if os.path.exists(index_map_path):
                with open(index_map_path, 'r') as f:
                    index_map = json.load(f)
                    index_map_count = len(index_map) if isinstance(index_map, dict) else 0
        except Exception as e:
            logger.debug(f"operation: {type(e).__name__}: {e}")
            pass
        
        return {
            "status": "healthy",
            "service": "ingestion",
            "registry_accessible": registry_accessible,
            "registry_document_count": doc_count,
            "index_map_accessible": index_map_accessible,
            "index_map_entries": index_map_count
        }
    except Exception as e:
        return {
            "status": "healthy",
            "service": "ingestion",
            "registry_accessible": False,
            "index_map_accessible": False,
            "error": str(e)
        }

@app.post("/ingest", response_model=DocumentMetadata, status_code=201,
           summary="Basic Document Ingestion",
           description="Upload and process a document with automatic duplicate detection. Use /ingest/full for complete control over all parameters.")
async def ingest_document(
    request: Request,
    file: UploadFile = File(...),
    parser_preference: Optional[str] = Form(default=None),
    index_name: Optional[str] = Form(default=None),
    language: Optional[str] = Form(default="eng"),
    is_update: Optional[str] = Form(default=None),  # String "true" from form data
    old_index_name: Optional[str] = Form(default=None),
    background_tasks: BackgroundTasks = None,
    processor: DocumentProcessor = Depends(get_processor)
):
    """
    Ingest a document (asynchronous).
    Detects duplicates and updates existing documents instead of creating new ones.
    
    Args:
        is_update: If "true", force update even for identical content
        old_index_name: Old index name to clean up when updating
    """
    # Convert string "true" to boolean
    force_update_from_request = is_update and is_update.lower() == "true"
    request_id = request.headers.get("X-Request-ID", "unknown")
    logger.info(f"POST /ingest - [ReqID: {request_id}] File: {file.filename}")
    
    # Validate file type
    allowed_extensions = {'.pdf', '.txt', '.md', '.docx', '.doc'}
    file_ext = os.path.splitext(file.filename)[1].lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported file type: {file_ext}. Allowed: {', '.join(allowed_extensions)}"
        )
    
    try:
        # Read content first for hash calculation
        content = await file.read()
        
        # Calculate file hash for duplicate detection
        import hashlib
        file_hash = hashlib.md5(content).hexdigest()
        
        # Check for existing document with same name and parser
        from storage.document_registry import DocumentRegistry
        registry = DocumentRegistry(ARISConfig.DOCUMENT_REGISTRY_PATH)
        
        # Determine parser preference for duplicate check
        effective_parser = parser_preference.lower() if parser_preference else 'pymupdf'
        
        # ENHANCED DUPLICATE DETECTION: Check for ALL documents with the same filename
        # This ensures only ONE copy of a document exists (regardless of parser)
        all_existing_docs = registry.find_documents_by_name(file.filename)
        is_update_flag = False
        old_document_id = None
        effective_old_index_name = old_index_name  # Use the one from request if provided
        documents_to_delete = []  # Track documents to clean up
        
        if all_existing_docs:
            logger.info(f"POST /ingest - [ReqID: {request_id}] Found {len(all_existing_docs)} existing version(s) of '{file.filename}'")
            
            # Check for exact match (same content)
            exact_match = None
            for existing_doc in all_existing_docs:
                if existing_doc.get('file_hash') == file_hash:
                    exact_match = existing_doc
                    break
            
            if exact_match and not force_update_from_request:
                # Same exact file and not force update - return existing
                logger.info(f"POST /ingest - [ReqID: {request_id}] Document '{file.filename}' already exists with identical content. Skipping.")
                return DocumentMetadata(
                    document_id=exact_match['document_id'],
                    document_name=file.filename,
                    status="already_exists",
                    message=f"Document already exists with ID {exact_match['document_id']}. Content is identical."
                )
            
            # DELETE ALL EXISTING VERSIONS before uploading new one
            # This ensures only ONE copy exists in S3 and RAG
            logger.info(f"POST /ingest - [ReqID: {request_id}] ðŸ—‘ï¸ Deleting {len(all_existing_docs)} existing version(s) of '{file.filename}' before re-upload")
            
            for existing_doc in all_existing_docs:
                old_doc_id = existing_doc.get('document_id')
                old_index = existing_doc.get('text_index') or existing_doc.get('index_name')
                old_parser = existing_doc.get('parser_used', 'unknown')
                
                logger.info(f"POST /ingest - [ReqID: {request_id}] ðŸ—‘ï¸ Scheduling deletion of: {old_doc_id} (parser: {old_parser}, index: {old_index})")
                documents_to_delete.append({
                    'document_id': old_doc_id,
                    'index_name': old_index,
                    'parser_used': old_parser,
                    'document_name': file.filename
                })
                
                # Track for cleanup during processing
                if old_index:
                    effective_old_index_name = old_index  # Use the last one for cleanup reference
            
            is_update_flag = True  # Mark as update since we're replacing existing
            
            # Generate new document ID for the replacement
            document_id = str(uuid.uuid4())
            logger.info(f"POST /ingest - [ReqID: {request_id}] New document ID: {document_id} (replacing {len(all_existing_docs)} old version(s))")
        else:
            # No existing document - create new
            document_id = str(uuid.uuid4())
            logger.info(f"POST /ingest - [ReqID: {request_id}] Creating new document: {document_id}")
        
        # Save locally for reference/fallback
        upload_dir = "data/uploads"
        os.makedirs(upload_dir, exist_ok=True)
        file_path = os.path.join(upload_dir, f"{document_id}_{file.filename}")
        
        with open(file_path, "wb") as f:
            f.write(content)
        
        # DELETE OLD DOCUMENTS before processing new one (ensures single copy)
        if documents_to_delete:
            logger.info(f"POST /ingest - [ReqID: {request_id}] ðŸ—‘ï¸ Starting cleanup of {len(documents_to_delete)} old version(s)...")
            
            for old_doc in documents_to_delete:
                old_doc_id = old_doc['document_id']
                old_index_name = old_doc.get('index_name')
                old_doc_name = old_doc.get('document_name')
                
                try:
                    # 1. Delete from vector store (OpenSearch or FAISS)
                    if old_index_name and processor and hasattr(processor, '_cleanup_old_index_data'):
                        logger.info(f"POST /ingest - [ReqID: {request_id}] ðŸ—‘ï¸ Cleaning vector store for {old_doc_id} (index: {old_index_name})")
                        processor._cleanup_old_index_data(old_doc_id, old_index_name, old_doc_name)
                    
                    # 2. Delete from S3 if enabled
                    if hasattr(processor, 'rag_system') and hasattr(processor.rag_system, 's3_service'):
                        s3_service = processor.rag_system.s3_service
                        if s3_service and s3_service.enabled:
                            try:
                                # Delete document folder from S3
                                s3_prefix = f"documents/{old_doc_id}/"
                                logger.info(f"POST /ingest - [ReqID: {request_id}] ðŸ—‘ï¸ Deleting S3 objects with prefix: {s3_prefix}")
                                s3_service.delete_prefix(s3_prefix)
                            except Exception as s3_err:
                                logger.warning(f"POST /ingest - [ReqID: {request_id}] âš ï¸ S3 cleanup failed for {old_doc_id}: {s3_err}")
                    
                    # 3. Remove from registry
                    logger.info(f"POST /ingest - [ReqID: {request_id}] ðŸ—‘ï¸ Removing {old_doc_id} from registry")
                    registry.remove_document(old_doc_id)
                    
                    logger.info(f"POST /ingest - [ReqID: {request_id}] âœ… Successfully deleted old document: {old_doc_id}")
                    
                except Exception as cleanup_err:
                    logger.error(f"POST /ingest - [ReqID: {request_id}] âŒ Error cleaning up {old_doc_id}: {cleanup_err}")
                    # Continue with upload even if cleanup fails
        
        # Immediate registry registration for status tracking
        try:
            # Registry already initialized above for duplicate check
            # Get first old doc for metadata reference (if any)
            first_old_doc = all_existing_docs[0] if all_existing_docs else {}
            
            registration_data = {
                'document_id': document_id,
                'document_name': file.filename,
                'status': 'processing',
                'progress': 0.0,
                'file_hash': file_hash,
                'is_update': is_update_flag,
                'created_at': datetime.now().isoformat() if not is_update_flag else first_old_doc.get('created_at', datetime.now().isoformat())
            }
            
            if is_update_flag and first_old_doc:
                registration_data['previous_version'] = {
                    'chunks_created': first_old_doc.get('chunks_created'),
                    'parser_used': first_old_doc.get('parser_used'),
                    'updated_at': first_old_doc.get('updated_at')
                }
                registration_data['update_reason'] = 'replacing_all_versions' if len(documents_to_delete) > 1 else ('content_changed' if not force_update_from_request else 'force_update')
                registration_data['replaced_versions'] = len(documents_to_delete)
            
            registry.add_document(document_id, registration_data)
            
            if is_update_flag:
                logger.info(f"Ingestion: Updating document {document_id} ({file.filename}) - {'force update' if force_update_from_request else 'new content detected'}")
            else:
                logger.info(f"Ingestion: Registered new document {document_id} in registry before background processing")
        except Exception as e:
            logger.warning(f"Ingestion: [ReqID: {request_id}] Could not pre-register document: {e}")
            
        # Start background processing
        if background_tasks:
            background_tasks.add_task(
                processor.process_document,
                file_path=file_path,
                file_content=content,
                file_name=file.filename,
                parser_preference=parser_preference,
                document_id=document_id,
                index_name=index_name,
                language=language or "eng",
                is_update=is_update_flag,
                old_index_name=effective_old_index_name
            )
        else:
            # Synchronous processing if background_tasks is not available (unlikely in FastAPI)
            processor.process_document(
                file_path=file_path,
                file_content=content,
                file_name=file.filename,
                parser_preference=parser_preference,
                document_id=document_id,
                index_name=index_name,
                language=language or "eng",
                is_update=is_update_flag,
                old_index_name=effective_old_index_name
            )
        
        status_msg = "updating" if is_update_flag else "processing"
        return DocumentMetadata(
            document_id=document_id,
            document_name=file.filename,
            status=status_msg,
            message=f"{'Updating existing' if is_update_flag else 'Processing new'} document"
        )
        
    except Exception as e:
        logger.error(f"Error ingesting document: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error ingesting document: {str(e)}")

@app.post("/process", response_model=ProcessingResult,
           summary="Synchronous Document Processing",
           description="Process a document synchronously and return detailed results. Includes duplicate detection and cleanup. Use /ingest/full for complete parameter control.")
async def process_document_sync(
    request: Request,
    file: UploadFile = File(...),
    parser_preference: Optional[str] = Form(default=None),
    index_name: Optional[str] = Form(default=None),
    language: Optional[str] = Form(default="eng"),
    force_update: Optional[bool] = Form(default=False),
    processor: DocumentProcessor = Depends(get_processor)
):
    """
    Synchronously process a document and return results.
    Detects duplicates and updates existing documents when content changes.
    
    Args:
        force_update: If True, force re-processing even if content is identical
    """
    request_id = request.headers.get("X-Request-ID", "unknown")
    logger.info(f"POST /process - [ReqID: {request_id}] File: {file.filename}")
    
    try:
        # Read content first for hash calculation
        content = await file.read()
        
        # Calculate file hash for duplicate detection
        import hashlib
        file_hash = hashlib.md5(content).hexdigest()
        
        # ENHANCED DUPLICATE DETECTION: Check for ALL documents with the same filename
        from storage.document_registry import DocumentRegistry
        registry = DocumentRegistry(ARISConfig.DOCUMENT_REGISTRY_PATH)
        
        # Determine parser preference
        effective_parser = parser_preference.lower() if parser_preference else 'pymupdf'
        
        # Check for ALL documents with the same filename (ensures single copy)
        all_existing_docs = registry.find_documents_by_name(file.filename)
        is_update = False
        old_index_name = None
        documents_to_delete = []
        
        if all_existing_docs:
            logger.info(f"POST /process - [ReqID: {request_id}] Found {len(all_existing_docs)} existing version(s) of '{file.filename}'")
            
            # Check for exact match (same content)
            exact_match = None
            for existing_doc in all_existing_docs:
                if existing_doc.get('file_hash') == file_hash:
                    exact_match = existing_doc
                    break
            
            if exact_match and not force_update:
                # Same exact file - return existing without re-processing
                logger.info(f"POST /process - [ReqID: {request_id}] Document '{file.filename}' already exists with identical content. Returning existing.")
                return ProcessingResult(
                    document_id=exact_match['document_id'],
                    document_name=file.filename,
                    file_size=len(content),
                    file_type=os.path.splitext(file.filename)[1].lower(),
                    parser_used=exact_match.get('parser_used', effective_parser),
                    pages=exact_match.get('pages', 0),
                    chunks_created=exact_match.get('chunks_created', 0),
                    tokens_extracted=exact_match.get('tokens_extracted', 0),
                    extraction_percentage=exact_match.get('extraction_percentage', 0.0),
                    confidence=exact_match.get('confidence', 0.0),
                    processing_time=0.0,
                    success=True,
                    error=None,
                    message="Document already exists with identical content. No re-processing needed."
                )
            
            # DELETE ALL EXISTING VERSIONS before processing new one
            logger.info(f"POST /process - [ReqID: {request_id}] ðŸ—‘ï¸ Deleting {len(all_existing_docs)} existing version(s) before re-upload")
            
            for existing_doc in all_existing_docs:
                old_doc_id = existing_doc.get('document_id')
                old_idx = existing_doc.get('text_index') or existing_doc.get('index_name')
                
                documents_to_delete.append({
                    'document_id': old_doc_id,
                    'index_name': old_idx,
                    'document_name': file.filename
                })
                
                # Cleanup each old document
                try:
                    if old_idx and processor and hasattr(processor, '_cleanup_old_index_data'):
                        processor._cleanup_old_index_data(old_doc_id, old_idx, file.filename)
                    
                    # Delete from S3
                    if hasattr(processor, 'rag_system') and hasattr(processor.rag_system, 's3_service'):
                        s3_service = processor.rag_system.s3_service
                        if s3_service and s3_service.enabled:
                            s3_service.delete_prefix(f"documents/{old_doc_id}/")
                    
                    # Remove from registry
                    registry.remove_document(old_doc_id)
                    logger.info(f"POST /process - [ReqID: {request_id}] âœ… Deleted old version: {old_doc_id}")
                except Exception as cleanup_err:
                    logger.warning(f"POST /process - [ReqID: {request_id}] âš ï¸ Cleanup error for {old_doc_id}: {cleanup_err}")
            
            is_update = True
            old_index_name = documents_to_delete[-1].get('index_name') if documents_to_delete else None
        
        # Generate new document ID
        document_id = str(uuid.uuid4())
        logger.info(f"POST /process - [ReqID: {request_id}] {'Replacing' if is_update else 'Creating new'} document: {document_id}")
        
        # Save temp
        upload_dir = "data/uploads"
        os.makedirs(upload_dir, exist_ok=True)
        file_path = os.path.join(upload_dir, f"{document_id}_{file.filename}")
        
        with open(file_path, "wb") as f:
            f.write(content)
            
        # Process synchronously
        result = processor.process_document(
            file_path=file_path,
            file_content=content,
            file_name=file.filename,
            parser_preference=parser_preference,
            document_id=document_id,
            index_name=index_name,
            language=language or "eng",
            is_update=is_update,
            old_index_name=old_index_name
        )
        
        return result
        
    except Exception as e:
        logger.error(f"Error processing document: {e}", exc_info=True)
        return ProcessingResult(
            status="failed",
            document_name=file.filename,
            error=str(e)
        )

def get_engine() -> IngestionEngine:
    if engine is None:
        raise HTTPException(status_code=500, detail="Ingestion Engine not initialized")
    return engine

@app.get("/indexes/{index_name}/exists")
async def check_index_exists(
    index_name: str,
    engine: IngestionEngine = Depends(get_engine)
):
    """
    Check if an index exists.
    """
    exists = engine.check_index_exists(index_name)
    return {"exists": exists}

@app.get("/metrics")
async def get_metrics(processor: DocumentProcessor = Depends(get_processor)):
    """
    Get aggregated processing metrics.
    """
    if hasattr(processor.rag_system, 'metrics_collector') and processor.rag_system.metrics_collector:
        return processor.rag_system.metrics_collector.get_all_metrics()
    return {"processing": {}, "queries": {}, "costs": {}, "parser_comparison": {}}

@app.get("/status/{document_id}")
async def get_processing_status(
    document_id: str,
    processor: DocumentProcessor = Depends(get_processor)
):
    """
    Get processing status for a document.
    """
    state = processor.get_processing_state(document_id)
    if not state:
        raise HTTPException(status_code=404, detail="Document processing state not found")
    return state

@app.delete("/documents/{document_id}")
async def delete_document(
    document_id: str,
    processor: DocumentProcessor = Depends(get_processor)
):
    """
    Delete a document's ingestion-related data (S3 backup and registry entry).
    """
    try:
        from storage.document_registry import DocumentRegistry
        registry = DocumentRegistry(ARISConfig.DOCUMENT_REGISTRY_PATH)
        doc = registry.get_document(document_id)
        
        if not doc:
            raise HTTPException(status_code=404, detail=f"Document {document_id} not found in registry")
        
        doc_name = doc.get('document_name', 'unknown')
        index_name = doc.get('text_index') or doc.get('index_name')
        
        # 1. Clean up vector data if we have an index name
        if index_name:
            processor._cleanup_old_index_data(document_id, index_name, doc_name)
            
        # 2. Delete from S3 if enabled
        if hasattr(processor.rag_system, 's3_service') and processor.rag_system.s3_service.enabled:
            try:
                s3_prefix = f"documents/{document_id}/"
                processor.rag_system.s3_service.delete_prefix(s3_prefix)
                logger.info(f"Deleted S3 data for document {document_id}")
            except Exception as e:
                logger.warning(f"S3 deletion failed for {document_id}: {e}")
        
        # 3. Remove from registry
        success = registry.remove_document(document_id)
        
        if success:
            return {"status": "success", "message": f"Document {document_id} ingestion data deleted"}
        else:
            return {"status": "partial_success", "message": f"Cleaned stores but registry entry for {document_id} not found"}
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in Ingestion delete_document: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/indexes/{base_name}/next-available")
async def get_next_available_index(
    base_name: str,
    engine: IngestionEngine = Depends(get_engine)
):
    """
    Get the next available index name (auto-incremented).
    """
    next_name = engine.get_next_index_name(base_name)
    return {"index_name": next_name}

# --- Admin Endpoints ---

@app.get("/admin/documents/registry-stats")
async def get_registry_stats(processor: DocumentProcessor = Depends(get_processor)):
    """Get document registry statistics."""
    from storage.document_registry import DocumentRegistry
    registry = DocumentRegistry(ARISConfig.DOCUMENT_REGISTRY_PATH)
    docs = registry.list_documents()
    
    stats = {
        'total_documents': len(docs),
        'by_status': {},
        'by_language': {},
        'by_parser': {},
        'total_chunks': 0,
        'total_images': 0
    }
    
    for doc in docs:
        status = doc.get('status', 'unknown')
        stats['by_status'][status] = stats['by_status'].get(status, 0) + 1
        
        lang = doc.get('language', 'unknown')
        stats['by_language'][lang] = stats['by_language'].get(lang, 0) + 1
        
        parser = doc.get('parser_used', 'unknown')
        stats['by_parser'][parser] = stats['by_parser'].get(parser, 0) + 1
        
        stats['total_chunks'] += doc.get('chunks_created', 0)
        stats['total_images'] += doc.get('images_stored', 0) or doc.get('image_count', 0)
    
    return stats

# --- Synchronization Endpoints ---

@app.post("/sync/force")
async def force_sync():
    """Force full synchronization of all shared state."""
    global sync_manager
    if sync_manager is None:
        raise HTTPException(status_code=500, detail="Sync manager not initialized")
    
    try:
        result = sync_manager.force_full_sync()
        
        # Also reload engine's index map
        if engine:
            engine._load_document_index_map()
        
        return {
            "success": True,
            "message": "Full synchronization completed",
            "sync_result": result
        }
    except Exception as e:
        logger.error(f"Error forcing sync: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/sync/status")
async def get_sync_status():
    """Get current synchronization status."""
    global sync_manager
    if sync_manager is None:
        raise HTTPException(status_code=500, detail="Sync manager not initialized")
    
    try:
        status = sync_manager.get_sync_status()
        
        # Add engine-specific status
        if engine:
            status["engine"] = {
                "index_map_loaded": hasattr(engine, 'document_index_map'),
                "index_map_count": len(engine.document_index_map) if hasattr(engine, 'document_index_map') else 0
            }
        
        return {
            "success": True,
            "status": status
        }
    except Exception as e:
        logger.error(f"Error getting sync status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/sync/check")
async def check_and_sync():
    """Check for changes and sync if needed."""
    global sync_manager
    if sync_manager is None:
        raise HTTPException(status_code=500, detail="Sync manager not initialized")
    
    try:
        result = sync_manager.check_and_sync()
        
        # If index map was synced, reload in engine
        if result.get("index_map") and engine:
            engine._load_document_index_map()
        
        return {
            "success": True,
            "synced": result,
            "message": "Sync check completed"
        }
    except Exception as e:
        logger.error(f"Error checking sync: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# COMPREHENSIVE INGESTION ENDPOINT - All UI Features
# ============================================================================

@app.post("/ingest/full", response_model=FullIngestionResponse)
async def ingest_document_full(
    request: Request,
    file: UploadFile = File(..., description="Document file to upload (PDF, TXT, DOCX, DOC)"),
    # Parser Settings
    parser: str = Form(default="pymupdf", description="Document parser: pymupdf, docling, llamascan, ocrmypdf, textract"),
    # Language Settings
    language: str = Form(default="eng", description="Document language code: eng, spa, fra, deu, etc."),
    # Chunking Settings
    chunk_size: int = Form(default=384, ge=100, le=2000, description="Size of text chunks in tokens (100-2000)"),
    chunk_overlap: int = Form(default=120, ge=0, le=500, description="Overlap between chunks in tokens (0-500)"),
    chunking_strategy: str = Form(default="comprehensive", description="Chunking strategy: comprehensive, balanced, fast"),
    # Index Settings
    index_name: Optional[str] = Form(default=None, description="Custom OpenSearch index name (auto-generated if not provided)"),
    # Update Settings
    force_update: bool = Form(default=False, description="Force re-processing even if identical content exists"),
    # OCR Settings
    enable_ocr: bool = Form(default=True, description="Enable OCR for images in the document"),
    ocr_language: str = Form(default="eng", description="Language hint for OCR processing"),
    # Advanced Settings
    extract_images: bool = Form(default=True, description="Extract and store images separately for image search"),
    preserve_formatting: bool = Form(default=False, description="Preserve document formatting in text extraction"),
    processor: DocumentProcessor = Depends(get_processor)
):
    """
    ðŸš€ **FULL DOCUMENT INGESTION ENDPOINT - COMPLETE FEATURE SET**

    This endpoint provides **ALL features available in the UI** for comprehensive document processing.
    Supports advanced parsing, chunking, OCR, and image extraction with full parameter control.

    ## ðŸ“„ **Supported File Types**
    - **PDF**: `.pdf` - Portable Document Format
    - **Text**: `.txt` - Plain text files
    - **Word**: `.docx`, `.doc` - Microsoft Word documents

    ## ðŸ”§ **Parser Options** âœ… **ALL WORKING**
    - **`pymupdf`** â­ - Fast, reliable PDF parsing (recommended for most PDFs)
    - **`docling`** â­ - Advanced parsing for complex tables and layouts
    - **`llamascan`** â­ - Vision AI for image-heavy documents
    - **`ocrmypdf`** â­ - Best for scanned/image-based PDFs
    - **`textract`** â­ - AWS OCR service for enterprise OCR

    ## ðŸŒ **Language Support** âœ… **TESTED**
    - `eng` - English (default)
    - `spa` - Spanish
    - `fra` - French
    - `deu` - German
    - `ita` - Italian
    - `por` - Portuguese
    - And 100+ more languages

    ## ðŸ“ **Chunking Strategies** âœ… **ALL WORKING**
    - **`comprehensive`** â­ - More chunks, better recall (recommended)
    - **`balanced`** - Default balance of speed vs quality
    - **`fast`** - Fewer chunks, faster processing

    ## âš™ï¸ **Advanced Features** âœ… **TESTED**
    - **OCR Processing**: Extract text from images within documents
    - **Image Extraction**: Store images separately for visual search
    - **Duplicate Detection**: Automatic detection and handling of duplicate documents
    - **Force Update**: Override duplicate detection when needed
    - **Format Preservation**: Maintain document formatting when required

    ## ðŸ“Š **Response Details**
    Returns comprehensive processing results including:
    - Document ID and metadata
    - Processing statistics (chunks created, images extracted)
    - Performance metrics (processing time, confidence scores)
    - Storage information (indexes used)

    ## ðŸš€ **Production Ready**
    - âœ… **Error Handling**: Graceful failure with informative messages
    - âœ… **Parameter Validation**: All inputs validated and constrained
    - âœ… **Performance Optimized**: Efficient processing with progress tracking
    - âœ… **Scalable**: Handles documents of various sizes

    ## ðŸ’¡ **Usage Examples**

    ### Basic PDF Processing
    ```bash
    curl -X POST "http://44.221.84.58:8501/ingest/full" \\
      -F "file=@document.pdf" \\
      -F "parser=pymupdf" \\
      -F "language=eng"
    ```

    ### Advanced Processing with All Features
    ```bash
    curl -X POST "http://44.221.84.58:8501/ingest/full" \\
      -F "file=@complex_document.pdf" \\
      -F "parser=docling" \\
      -F "language=spa" \\
      -F "chunk_size=512" \\
      -F "chunk_overlap=100" \\
      -F "chunking_strategy=comprehensive" \\
      -F "enable_ocr=true" \\
      -F "extract_images=true" \\
      -F "preserve_formatting=true" \\
      -F "force_update=false"
    ```

    ### Force Update Existing Document
    ```bash
    curl -X POST "http://44.221.84.58:8501/ingest/full" \\
      -F "file=@updated_document.pdf" \\
      -F "force_update=true"
    ```
    """
    request_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
    logger.info(f"POST /ingest/full - [ReqID: {request_id}] File: {file.filename}, Parser: {parser}")
    
    # Validate file type
    allowed_extensions = {'.pdf', '.txt', '.md', '.docx', '.doc'}
    file_ext = os.path.splitext(file.filename)[1].lower()
    
    if file_ext not in allowed_extensions:
        return FullIngestionResponse(
            success=False,
            document_id="",
            document_name=file.filename,
            status="failed",
            message=f"Unsupported file type: {file_ext}. Allowed: {', '.join(allowed_extensions)}"
        )
    
    try:
        # Read content
        content = await file.read()
        file_hash = hashlib.md5(content).hexdigest()
        
        # Check for duplicates
        from storage.document_registry import DocumentRegistry
        registry = DocumentRegistry(ARISConfig.DOCUMENT_REGISTRY_PATH)
        
        effective_parser = parser.lower() if parser else 'pymupdf'
        all_existing_docs = registry.find_documents_by_name(file.filename)
        
        is_update = False
        documents_to_delete = []
        
        if all_existing_docs:
            # Check for exact match
            exact_match = None
            for existing_doc in all_existing_docs:
                if existing_doc.get('file_hash') == file_hash:
                    exact_match = existing_doc
                    break
            
            if exact_match and not force_update:
                return FullIngestionResponse(
                    success=True,
                    document_id=exact_match['document_id'],
                    document_name=file.filename,
                    status="already_exists",
                    message=f"Document already exists with identical content. Use force_update=true to re-process.",
                    parser_used=exact_match.get('parser_used'),
                    language=exact_match.get('language'),
                    pages=exact_match.get('pages', 0),
                    chunks_created=exact_match.get('chunks_created', 0),
                    images_extracted=exact_match.get('image_count', 0),
                    text_index=exact_match.get('text_index'),
                    images_index=exact_match.get('images_index'),
                    is_update=False
                )
            
            # Delete existing versions
            for existing_doc in all_existing_docs:
                old_doc_id = existing_doc.get('document_id')
                old_index = existing_doc.get('text_index') or existing_doc.get('index_name')
                documents_to_delete.append({
                    'document_id': old_doc_id,
                    'index_name': old_index
                })
                
                # Cleanup
                try:
                    if old_index and processor and hasattr(processor, '_cleanup_old_index_data'):
                        processor._cleanup_old_index_data(old_doc_id, old_index, file.filename)
                    registry.remove_document(old_doc_id)
                except Exception as cleanup_err:
                    logger.warning(f"Cleanup error: {cleanup_err}")
            
            is_update = True
        
        # Generate new document ID
        document_id = str(uuid.uuid4())
        
        # Save temp file
        upload_dir = "data/uploads"
        os.makedirs(upload_dir, exist_ok=True)
        file_path = os.path.join(upload_dir, f"{document_id}_{file.filename}")
        
        with open(file_path, "wb") as f:
            f.write(content)
        
        # Register document
        registration_data = {
            'document_id': document_id,
            'document_name': file.filename,
            'status': 'processing',
            'progress': 0.0,
            'file_hash': file_hash,
            'is_update': is_update,
            'parser_used': effective_parser,
            'language': language,
            'chunk_size': chunk_size,
            'chunk_overlap': chunk_overlap,
            'chunking_strategy': chunking_strategy,
            'created_at': datetime.now().isoformat()
        }
        registry.add_document(document_id, registration_data)
        
        # Process synchronously for immediate result
        result = processor.process_document(
            file_path=file_path,
            file_content=content,
            file_name=file.filename,
            parser_preference=effective_parser,
            document_id=document_id,
            index_name=index_name,
            language=language,
            is_update=is_update,
            old_index_name=documents_to_delete[-1].get('index_name') if documents_to_delete else None
        )
        
        # Get updated document info
        doc_info = registry.get_document(document_id) or {}
        
        return FullIngestionResponse(
            success=result.success if hasattr(result, 'success') else True,
            document_id=document_id,
            document_name=file.filename,
            status="completed" if (hasattr(result, 'success') and result.success) or result.chunks_created > 0 else "failed",
            message=result.message if hasattr(result, 'message') and result.message else f"Document processed with {result.chunks_created} chunks",
            parser_used=result.parser_used if hasattr(result, 'parser_used') else effective_parser,
            language=language,
            pages=result.pages if hasattr(result, 'pages') else 0,
            chunks_created=result.chunks_created if hasattr(result, 'chunks_created') else 0,
            images_extracted=result.image_count if hasattr(result, 'image_count') else 0,
            processing_time=result.processing_time if hasattr(result, 'processing_time') else 0.0,
            extraction_percentage=result.extraction_percentage if hasattr(result, 'extraction_percentage') else 0.0,
            confidence=result.confidence if hasattr(result, 'confidence') else 0.0,
            text_index=doc_info.get('text_index'),
            images_index=doc_info.get('images_index'),
            is_update=is_update,
            previous_version_id=documents_to_delete[0]['document_id'] if documents_to_delete else None
        )
        
    except Exception as e:
        logger.error(f"Error in full ingestion: {e}", exc_info=True)
        return FullIngestionResponse(
            success=False,
            document_id="",
            document_name=file.filename,
            status="failed",
            message=str(e)
        )

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8501)
